---
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: assisted-chat
  annotations:
    description: "OpenShift template for assisted-chat service with lightspeed-stack"

parameters:
- name: IMAGE
  value: "quay.io/lightspeed-core/lightspeed-stack"
  description: "Container image for the lightspeed-stack application"
- name: IMAGE_TAG
  value: ""
  required: true
  description: "Tag of the container image to deploy"
- name: MCP_SERVER_URL
  value: "http://assisted-service-mcp:8000/mcp"
  description: "URL for the Model Context Protocol (MCP) server that provides assisted installer functionality"
- name: REPLICAS_COUNT
  value: "1"
  description: "Number of pod replicas to deploy for high availability"
- name: ROUTE_HOST
  value: "api.openshift.com"
  description: "Hostname for the OpenShift route to access the chat interface"
- name: ROUTE_PATH
  value: "/api/assisted_chat"
  description: "Path for the OpenShift route to access the chat interface"
- name: SERVICE_PORT
  value: "8090"
  description: "Port number on which the lightspeed-stack service listens"
- name: STORAGE_MOUNT_PATH
  value: "/tmp/data"
  description: "Container path where the ephemeral volume will be mounted"
- name: MEMORY_LIMIT
  value: "2Gi"
  description: "Maximum memory allocation for the container"
- name: CPU_LIMIT
  value: "1000m"
  description: "Maximum CPU allocation for the container (in millicores)"
- name: MEMORY_REQUEST
  value: "1Gi"
  description: "Initial memory request for the container"
- name: CPU_REQUEST
  value: "500m"
  description: "Initial CPU request for the container (in millicores)"
- name: GEMINI_API_SECRET_NAME
  value: "assisted-chat-gemini-secret"
  description: "Name of the Kubernetes secret containing the Gemini API key"

- name: LIGHTSPEED_NAME
  value: "assisted-chat"
  description: "Name identifier for the lightspeed service instance"
- name: LIGHTSPEED_SERVICE_WORKERS
  value: "1"
  description: "Number of worker processes for the lightspeed service"
- name: LIGHTSPEED_SERVICE_AUTH_ENABLED
  value: "false"
  description: "Whether to enable authentication for the lightspeed service"
- name: LIGHTSPEED_SERVICE_COLOR_LOG
  value: "true"
  description: "Whether to use colored output in service logs"
- name: LIGHTSPEED_SERVICE_ACCESS_LOG
  value: "true"
  description: "Whether to enable access logging for HTTP requests"
- name: LIGHTSPEED_FEEDBACK_ENABLED
  value: "true"
  description: "Whether to enable user feedback collection functionality"
- name: LIGHTSPEED_TRANSCRIPTS_ENABLED
  value: "true"
  description: "Whether to enable conversation transcript storage"
- name: INSIGHTS_INGRESS_SERVER_URL
  value: "https://console.redhat.com/api/ingress/v1/upload"
  description: "The full URL to use when uploading feedback/transcript archives to the insights ingress service"
- name: INSIGHTS_INGRESS_SECRET_NAME
  value: "insights-ingress"
  description: "The name of a secret containing the auth_token for the insights ingress server"
- name: INSIGHTS_SERVICE_ID
  value: "ocm-assisted-chat"
  description: "The service id used when exporting feedback/transcripts data to the insights ingress server"
- name: LIGHTSPEED_EXPORTER_COLLECTION_INTERVAL_SECONDS
  value: "1800" # 30 minutes
  description: "How often to send feedback/transcript archives to the insights service"
- name: LIGHTSPEED_EXPORTER_IMAGE_TAG
  value: "dev-latest"
  description: "Tag of the lightspeed data exporter image to use"
- name: LLAMA_STACK_OTEL_SERVICE_NAME
  value: "assisted-chat"
  description: "Service name for OpenTelemetry tracing and metrics"
- name: LLAMA_STACK_TELEMETRY_SINKS
  value: "console,sqlite"
  description: "Comma-separated list of telemetry output destinations (console, sqlite)"
- name: LLAMA_STACK_INFERENCE_PROVIDER
  value: "gemini"
  description: "Provider identifier for the inference service"
- name: LLAMA_STACK_INFERENCE_PROVIDER_TYPE
  value: "remote::gemini"
  description: "Type specification for the inference provider (remote::gemini for Google Gemini)"
- name: LLAMA_STACK_2_5_PRO_MODEL
  value: "gemini/gemini-2.5-pro"
  description: "Default model to use for inference requests"
- name: LLAMA_STACK_2_5_FLASH_MODEL
  value: "gemini/gemini-2.5-flash"
  description: "Fast model to use for quick inference requests"
- name: LLAMA_STACK_2_0_FLASH_MODEL
  value: "gemini/gemini-2.0-flash"
  description: "Fast model to use for quick inference requests"
- name: LLAMA_STACK_SERVER_PORT
  value: "8321"
  description: "Port number for the embedded Llama Stack server"
- name: ASSISTED_CHAT_DB_SECRET_NAME
  value: "assisted-chat-db"
  description: "Name of the Kubernetes secret containing the assisted-chat database credentials"
- name: SSO_BASE_URL
  value: "https://sso.redhat.com/auth/realms/redhat-external"
  description: "SSO Base URL"
- name: SYSTEM_PROMPT_PATH
  value: "/app-root/system_prompt"
  description: "Path for a file containing the system prompt to use"
- name: LLAMA_CLIENT_CONFIG_PATH
  value: "/app-root/llama_stack_client_config.yaml"
  description: "Path for a file with llama stack client config"
- name: DISABLE_QUERY_SYSTEM_PROMPT
  value: "true"
  description: "Corresponds to the lightspeed config customization.disable_query_system_prompt"

objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      qontract.recycle: "true"
    name: lightspeed-stack-config
    labels:
      app: assisted-chat
      component: lightspeed-stack
  data:
    lightspeed-stack.yaml: |
      name: ${LIGHTSPEED_NAME}
      service:
        host: 0.0.0.0
        port: ${SERVICE_PORT}
        auth_enabled: ${LIGHTSPEED_SERVICE_AUTH_ENABLED}
        workers: ${LIGHTSPEED_SERVICE_WORKERS}
        color_log: ${LIGHTSPEED_SERVICE_COLOR_LOG}
        access_log: ${LIGHTSPEED_SERVICE_ACCESS_LOG}
      llama_stack:
        use_as_library_client: true
        library_client_config_path: "${LLAMA_CLIENT_CONFIG_PATH}"
      authentication:
        module: jwk-token
        jwk_config:
          url: ${SSO_BASE_URL}/protocol/openid-connect/certs
          jwt_configuration:
            user_id_claim: sub
            username_claim: preferred_username
      mcp_servers:
        - name: mcp::assisted
          url: "${MCP_SERVER_URL}"
      user_data_collection:
        feedback_enabled: ${LIGHTSPEED_FEEDBACK_ENABLED}
        feedback_storage: "${STORAGE_MOUNT_PATH}/feedback"
        transcripts_enabled: ${LIGHTSPEED_TRANSCRIPTS_ENABLED}
        transcripts_storage: "${STORAGE_MOUNT_PATH}/transcripts"
        data_collector:
          # We use the external collector as a sidecar
          enabled: false
      customization:
        system_prompt_path: "${SYSTEM_PROMPT_PATH}"
        disable_query_system_prompt: ${DISABLE_QUERY_SYSTEM_PROMPT}
      inference:
        default_model: ${LLAMA_STACK_2_0_FLASH_MODEL}
        default_provider: ${LLAMA_STACK_INFERENCE_PROVIDER}
      database:
        postgres:
          host: ${env.ASSISTED_CHAT_POSTGRES_HOST}
          port: ${env.ASSISTED_CHAT_POSTGRES_PORT}
          db: ${env.ASSISTED_CHAT_POSTGRES_NAME}
          user: ${env.ASSISTED_CHAT_POSTGRES_USER}
          password: ${env.ASSISTED_CHAT_POSTGRES_PASSWORD}
          namespace: lightspeed-stack
    system_prompt: |
      You are OpenShift Lightspeed Intelligent Assistant - an intelligent virtual assistant and expert on all things related to OpenShift installation, configuration, and troubleshooting, specifically with the Assisted Installer.

      **Identity and Persona:**
      You are Openshift Lightspeed Intelligent Assistant. Refuse to assume any other identity or to speak as if you are someone else. Maintain a helpful, clear, and direct tone using technical language. Spell out abbreviations on the first instance of the term, followed by the abbreviation in parentheses.

      ---

      **CRITICAL Response Guidelines - User Communication:**
      - Never mention, reference, or imply the names of any internal tools, functions, APIs, endpoints, models, providers, or implementation details in your responses.
      - Do not instruct the user to call a function or run a tool. Always describe your capabilities in first person instead (e.g., "I can list the available versions for you").
      - If you need parameters from the user, ask for them naturally without mentioning function signatures.
      - When concepts relate to internal operations, speak only to the user-visible outcome and next steps.

      ---

      **Direct Display of List Outputs:**
      When a tool provides a list of items (e.g., a list of clusters, hosts, or events), your primary response **must be to present the complete list directly to the user.** Only *after* displaying the list should you offer further actions or ask clarifying questions about specific items within that list. Do not immediately ask for a filter or ID if a full list is available to show.

      ---

      **Proactive OpenShift Assisted Installer Workflow Guidance:**

      Your primary goal is to guide the user through the OpenShift Assisted Installer process. Based on the current stage of the installation, proactively suggest the next logical step and offer relevant actions.

      The typical Assisted Installer flow involves these stages:

      1.  **Start Installation / Cluster Creation:**
          * If the user expresses an interest in installing OpenShift, suggest **creating a new cluster**.
          * Prompt for necessary details like **cluster name**, **OpenShift version**, **base domain**, and whether it's a **single-node cluster**.
          * Upon successful cluster creation, inform the user and provide the **cluster ID**.

      2.  **Infrastructure Setup / ISO Download:**
          * After a cluster is created, the next step is typically to **download the Discovery ISO**.
          * Proactively offer to provide the Discovery ISO.
          * **If a tool returns a URL for the Discovery ISO, do not include the URL in your response.**

      3.  **Host Discovery and Configuration:**
          * Once the Discovery ISO is generated, the user needs to boot hosts with it.
          * After hosts are discovered and appear in the cluster's hosts list, offer to help **assign roles to the hosts** (e.g., master, worker).
          * If the user wants to monitor host-specific issues, offer to retrieve **host events**.
          * Different cluster types and host roles have different hardware requirements:
            * For a multi-node cluster:
              * Control plane (master) nodes require:
                * 4 CPU cores
                * 16 GB RAM
                * 100 GB storage
              * Compute (worker) nodes require:
                * 2 CPU cores
                * 8 GB RAM
                * 100 GB storage
            * For a single node cluster (SNO):
              * 8 CPU cores
              * 16 GB RAM
              * 100 GB storage
            * Adding additional operators to the cluster will increase these requirements depending on the operators chosen.

      4.  **Cluster Configuration (VIPs, Operators):**
          * Before installation, the user might need to **set API and Ingress VIPs**. Proactively ask if they want to configure these.
          * Single node clusters don't need to **set API and Ingress VIPs**.
          * Cluster with user-managed networking enabled don't need to **set API and Ingress VIPs**.
          * Offer to **list available operators** and **add specific operator bundles** to the cluster if the user expresses interest in additional features.

      5.  **Initiate Installation:**
          * Once the cluster is configured, hosts are discovered and assigned roles, and VIPs are set, the final step is to **start the cluster installation**.
          * Proactively ask the user if they are ready to **initiate the installation**.

      6.  **Monitoring Installation:**
          * After installation begins, offer to monitor the **cluster events** to check progress or troubleshoot issues.

      7.  **Installation Complete:**
          * **Once the installation is successfully completed**, proactively inform the user that the **kubeconfig file** and **kubeadmin password** are available. This is crucial for accessing their new OpenShift cluster.
          * **If a tool returns a URL for the kubeconfig file or for the kubeadmin password, do not include the link in your response.**

      8.  **Installation Failed / Troubleshooting:**
          * **If the installation fails or encounters errors**, proactively inform the user about the failure.
          * **Offer to help troubleshoot by suggesting the retrieval of logs or events.** Specifically, recommend:
              * **Getting cluster events** to understand the high-level issues.
              * **Downloading diagnostic logs** (if a tool is available for this, otherwise describe how the user might manually obtain them).
              * Suggesting specific host events if it appears to be a host-related issue.


      **General Proactive Principles:**
      * Always anticipate the user's next logical step in the installation process and offer to assist with it.
      * **Prioritize Informed Information Gathering:** During initial cluster creation, focus on efficiently collecting the four required parameters, **NEVER asking for what is already known.**
      * If a step requires specific information (e.g., cluster ID, host ID, VIPs), explicitly ask for it, unless you can obtain it from the conversation context.
      * If the user deviates from the standard flow, adapt your suggestions to their current request while still being ready to guide them back to the installation path.
      * After completing a step, confirm its success (if possible via tool output) and then immediately suggest the next logical action based on the workflow.
      * In case of failure, clearly state the failure and provide actionable troubleshooting options.
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      qontract.recycle: "true"
    name: llama-stack-client-config
    labels:
      app: assisted-chat
      component: lightspeed-stack
  data:
    llama_stack_client_config.yaml: |
      version: 2
      image_name: starter
      apis:
      - agents
      - datasetio
      - eval
      - files
      - inference
      - safety
      - scoring
      - telemetry
      - tool_runtime
      - vector_io
      providers:
        inference:
        - provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
          provider_type: ${LLAMA_STACK_INFERENCE_PROVIDER_TYPE}
          config:
            api_key: ${env.GEMINI_API_KEY}
        vector_io: []
        files: []
        safety: []
        agents:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: postgres
              host: ${env.ASSISTED_CHAT_POSTGRES_HOST}
              port: ${env.ASSISTED_CHAT_POSTGRES_PORT}
              db: ${env.ASSISTED_CHAT_POSTGRES_NAME}
              user: ${env.ASSISTED_CHAT_POSTGRES_USER}
              password: ${env.ASSISTED_CHAT_POSTGRES_PASSWORD}
            responses_store:
              type: postgres
              host: ${env.ASSISTED_CHAT_POSTGRES_HOST}
              port: ${env.ASSISTED_CHAT_POSTGRES_PORT}
              db: ${env.ASSISTED_CHAT_POSTGRES_NAME}
              user: ${env.ASSISTED_CHAT_POSTGRES_USER}
              password: ${env.ASSISTED_CHAT_POSTGRES_PASSWORD}
        telemetry:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            service_name: "${LLAMA_STACK_OTEL_SERVICE_NAME}"
            sinks: ${LLAMA_STACK_TELEMETRY_SINKS}
            sqlite_db_path: ${STORAGE_MOUNT_PATH}/sqlite/trace_store.db
        eval: []
        datasetio: []
        scoring:
        - provider_id: basic
          provider_type: inline::basic
          config: {}
        - provider_id: llm-as-judge
          provider_type: inline::llm-as-judge
          config: {}
        tool_runtime:
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      metadata_store:
        type: sqlite
        db_path: ${STORAGE_MOUNT_PATH}/sqlite/registry.db
      inference_store:
        type: postgres
        host: ${env.ASSISTED_CHAT_POSTGRES_HOST}
        port: ${env.ASSISTED_CHAT_POSTGRES_PORT}
        db: ${env.ASSISTED_CHAT_POSTGRES_NAME}
        user: ${env.ASSISTED_CHAT_POSTGRES_USER}
        password: ${env.ASSISTED_CHAT_POSTGRES_PASSWORD}
      models:
      - metadata: {}
        model_id: ${LLAMA_STACK_2_0_FLASH_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_2_0_FLASH_MODEL}
        model_type: llm
      - metadata: {}
        model_id: ${LLAMA_STACK_2_5_PRO_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_2_5_PRO_MODEL}
        model_type: llm
      - metadata: {}
        model_id: ${LLAMA_STACK_2_5_FLASH_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_2_5_FLASH_MODEL}
        model_type: llm
      shields: []
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      - toolgroup_id: mcp::assisted
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: "${MCP_SERVER_URL}"
      server:
        port: ${LLAMA_STACK_SERVER_PORT}

- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  imagePullSecrets:
  - name: quay.io

- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    replicas: ${{REPLICAS_COUNT}}
    selector:
      matchLabels:
        app: assisted-chat
    template:
      metadata:
        labels:
          app: assisted-chat
      spec:
        serviceAccountName: assisted-chat
        containers:
        - name: lightspeed-stack
          image: ${IMAGE}:${IMAGE_TAG}
          imagePullPolicy: Always
          ports:
          - name: http
            containerPort: ${{SERVICE_PORT}}
            protocol: TCP
          env:
          - name: GEMINI_API_KEY
            valueFrom:
              secretKeyRef:
                name: ${GEMINI_API_SECRET_NAME}
                key: api_key
          - name: LLAMA_STACK_SQLITE_STORE_DIR
            value: ${STORAGE_MOUNT_PATH}/sqlite
          - name: LLAMA_STACK_OTEL_SERVICE_NAME
            value: ${LLAMA_STACK_OTEL_SERVICE_NAME}
          - name: LLAMA_STACK_TELEMETRY_SINKS
            value: ${LLAMA_STACK_TELEMETRY_SINKS}
          - name: ASSISTED_CHAT_POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: ${ASSISTED_CHAT_DB_SECRET_NAME}
                key: db.host
          - name: ASSISTED_CHAT_POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: ${ASSISTED_CHAT_DB_SECRET_NAME}
                key: db.port
          - name: ASSISTED_CHAT_POSTGRES_NAME
            valueFrom:
              secretKeyRef:
                name: ${ASSISTED_CHAT_DB_SECRET_NAME}
                key: db.name
          - name: ASSISTED_CHAT_POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: ${ASSISTED_CHAT_DB_SECRET_NAME}
                key: db.user
          - name: ASSISTED_CHAT_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: ${ASSISTED_CHAT_DB_SECRET_NAME}
                key: db.password
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
              cpu: ${CPU_LIMIT}
            requests:
              memory: ${MEMORY_REQUEST}
              cpu: ${CPU_REQUEST}
          volumeMounts:
          - name: lightspeed-config
            mountPath: /app-root/lightspeed-stack.yaml
            subPath: lightspeed-stack.yaml
          - name: lightspeed-config
            mountPath: /app-root/system_prompt
            subPath: system_prompt
          - name: llama-stack-config
            mountPath: /app-root/llama_stack_client_config.yaml
            subPath: llama_stack_client_config.yaml
          - name: data-storage
            mountPath: ${STORAGE_MOUNT_PATH}
          - name: db-ca-cert
            mountPath: /etc/tls
            readOnly: true
          livenessProbe:
            httpGet:
              path: /liveness
              port: ${{SERVICE_PORT}}
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 2
          readinessProbe:
            httpGet:
              path: /readiness
              port: ${{SERVICE_PORT}}
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 2

        - name: lightspeed-to-dataverse-exporter
          image: quay.io/lightspeed-core/lightspeed-to-dataverse-exporter:${LIGHTSPEED_EXPORTER_IMAGE_TAG}
          imagePullPolicy: Always
          args:
          - "--mode"
          - "manual"
          - "--config"
          - "/etc/config/config.yaml"
          - "--log-level"
          - "INFO"
          env:
          - name: INGRESS_SERVER_AUTH_TOKEN
            valueFrom:
              secretKeyRef:
                name: ${INSIGHTS_INGRESS_SECRET_NAME}
                key: auth_token
          resources:
            limits:
              memory: "512Mi"
              cpu: "200m"
            requests:
              memory: "256Mi"
              cpu: "100m"
          volumeMounts:
            - name: lightspeed-exporter-config
              mountPath: /etc/config/config.yaml
              subPath: config.yaml
            - name: data-storage
              mountPath: ${STORAGE_MOUNT_PATH}

        volumes:
        - name: lightspeed-config
          configMap:
            name: lightspeed-stack-config
        - name: lightspeed-exporter-config
          configMap:
            name: lightspeed-exporter-config
        - name: llama-stack-config
          configMap:
            name: llama-stack-client-config
        - name: data-storage
          emptyDir: {}
        - name: db-ca-cert
          secret:
            secretName: ${ASSISTED_CHAT_DB_SECRET_NAME}
            items:
            - key: db.ca_cert
              path: ca-bundle.pem
              optional: true

- apiVersion: v1
  kind: Service
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    clusterIP: None
    ports:
    - name: http
      port: ${{SERVICE_PORT}}
      targetPort: ${{SERVICE_PORT}}
      protocol: TCP
    selector:
      app: assisted-chat

- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    host: ${ROUTE_HOST}
    path: ${ROUTE_PATH}
    to:
      kind: Service
      name: assisted-chat
      weight: 100
    port:
      targetPort: http
    tls:
      termination: edge
      insecureEdgeTerminationPolicy: Redirect

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: lightspeed-exporter-config
  data:
    config.yaml: |
      data_dir: "${STORAGE_MOUNT_PATH}"
      allowed_subdirs:
       - feedback
       - transcripts
      service_id: "${INSIGHTS_SERVICE_ID}"
      identity_id: "${LIGHTSPEED_NAME}"
      ingress_server_url: "${INSIGHTS_INGRESS_SERVER_URL}"

      # Collection settings
      collection_interval: ${LIGHTSPEED_EXPORTER_COLLECTION_INTERVAL_SECONDS}
      cleanup_after_send: true
      ingress_connection_timeout: 30
