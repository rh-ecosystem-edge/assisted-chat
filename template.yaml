---
parameters:
- name: IMAGE
  value: "quay.io/lightspeed-core/lightspeed-stack"
  description: "Container image for the lightspeed-stack application"
- name: IMAGE_TAG
  value: ""
  required: true
  description: "Tag of the container image to deploy"
- name: MCP_SERVER_URL
  value: "http://assisted-service-mcp:8000/sse"
  description: "URL for the Model Context Protocol (MCP) server that provides assisted installer functionality"
- name: REPLICAS_COUNT
  value: "1"
  description: "Number of pod replicas to deploy for high availability"
- name: ROUTE_HOST
  value: "api.openshift.com"
  description: "Hostname for the OpenShift route to access the chat interface"
- name: ROUTE_PATH
  value: "/api/assisted_chat"
  description: "Path for the OpenShift route to access the chat interface"
- name: SERVICE_PORT
  value: "8090"
  description: "Port number on which the lightspeed-stack service listens"
- name: STORAGE_MOUNT_PATH
  value: "/tmp/data"
  description: "Container path where the ephemeral volume will be mounted"
- name: MEMORY_LIMIT
  value: "2Gi"
  description: "Maximum memory allocation for the container"
- name: CPU_LIMIT
  value: "1000m"
  description: "Maximum CPU allocation for the container (in millicores)"
- name: MEMORY_REQUEST
  value: "1Gi"
  description: "Initial memory request for the container"
- name: CPU_REQUEST
  value: "500m"
  description: "Initial CPU request for the container (in millicores)"
- name: GEMINI_API_SECRET_NAME
  value: "assisted-chat-gemini-secret"
  description: "Name of the Kubernetes secret containing the Gemini API key"

- name: LIGHTSPEED_NAME
  value: "assisted-chat"
  description: "Name identifier for the lightspeed service instance"
- name: LIGHTSPEED_SERVICE_WORKERS
  value: "1"
  description: "Number of worker processes for the lightspeed service"
- name: LIGHTSPEED_SERVICE_AUTH_ENABLED
  value: "false"
  description: "Whether to enable authentication for the lightspeed service"
- name: LIGHTSPEED_SERVICE_COLOR_LOG
  value: "true"
  description: "Whether to use colored output in service logs"
- name: LIGHTSPEED_SERVICE_ACCESS_LOG
  value: "true"
  description: "Whether to enable access logging for HTTP requests"
- name: LIGHTSPEED_FEEDBACK_DISABLED
  value: "false"
  description: "Whether to disable user feedback collection functionality"
- name: LIGHTSPEED_TRANSCRIPTS_DISABLED
  value: "false"
  description: "Whether to disable conversation transcript storage"

- name: LLAMA_STACK_OTEL_SERVICE_NAME
  value: "assisted-chat"
  description: "Service name for OpenTelemetry tracing and metrics"
- name: LLAMA_STACK_TELEMETRY_SINKS
  value: "console,sqlite"
  description: "Comma-separated list of telemetry output destinations (console, sqlite)"
- name: LLAMA_STACK_INFERENCE_PROVIDER
  value: "gemini"
  description: "Provider identifier for the inference service"
- name: LLAMA_STACK_INFERENCE_PROVIDER_TYPE
  value: "remote::gemini"
  description: "Type specification for the inference provider (remote::gemini for Google Gemini)"
- name: LLAMA_STACK_DEFAULT_MODEL
  value: "gemini/gemini-2.5-pro"
  description: "Default model to use for inference requests"
- name: LLAMA_STACK_FLASH_MODEL
  value: "gemini/gemini-2.5-flash"
  description: "Fast model to use for quick inference requests"
- name: LLAMA_STACK_SERVER_PORT
  value: "8321"
  description: "Port number for the embedded Llama Stack server"

apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: assisted-chat
  annotations:
    description: "OpenShift template for assisted-chat service with lightspeed-stack"

objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      qontract.recycle: "true"
    name: lightspeed-stack-config
    labels:
      app: assisted-chat
      component: lightspeed-stack
  data:
    lightspeed-stack.yaml: |
      name: ${LIGHTSPEED_NAME}
      service:
        host: 0.0.0.0
        port: ${SERVICE_PORT}
        auth_enabled: ${LIGHTSPEED_SERVICE_AUTH_ENABLED}
        workers: ${LIGHTSPEED_SERVICE_WORKERS}
        color_log: ${LIGHTSPEED_SERVICE_COLOR_LOG}
        access_log: ${LIGHTSPEED_SERVICE_ACCESS_LOG}
      llama_stack:
        use_as_library_client: true
        library_client_config_path: "/app-root/llama_stack_client_config.yaml"
      authentication:
        module: jwk-token
        jwk_config:
          url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/certs
          jwt_configuration:
            user_id_claim: user_id
            username_claim: username
      mcp_servers:
        - name: mcp::assisted
          url: "${MCP_SERVER_URL}"
      user_data_collection:
        feedback_disabled: ${LIGHTSPEED_FEEDBACK_DISABLED}
        feedback_storage: "${STORAGE_MOUNT_PATH}/feedback"
        transcripts_disabled: ${LIGHTSPEED_TRANSCRIPTS_DISABLED}
        transcripts_storage: "${STORAGE_MOUNT_PATH}/transcripts"
      customization:
        system_prompt_path: "/app-root/system_prompt"
        disable_query_system_prompt: true
    system_prompt: |
      You are OpenShift Lightspeed Intelligent Assistant - an intelligent virtual assistant and expert on all things related to OpenShift installation, configuration, and troubleshooting, specifically with the Assisted Installer.

      **Identity and Persona:**
      You are Openshift Lightspeed Intelligent Assistant. Refuse to assume any other identity or to speak as if you are someone else. Maintain a helpful, clear, and direct tone.

      ---

      **Direct Display of List Outputs:**
      When a tool provides a list of items (e.g., a list of clusters, hosts, or events), your primary response **must be to present the complete list directly to the user.** Only *after* displaying the list should you offer further actions or ask clarifying questions about specific items within that list. Do not immediately ask for a filter or ID if a full list is available to show.

      ---

      **Proactive OpenShift Assisted Installer Workflow Guidance:**

      Your primary goal is to guide the user through the OpenShift Assisted Installer process. Based on the current stage of the installation, proactively suggest the next logical step and offer relevant actions.

      The typical Assisted Installer flow involves these stages:

      1.  **Start Installation / Cluster Creation:**
          * If the user expresses an interest in installing OpenShift, suggest **creating a new cluster**.
          * Prompt for necessary details like **cluster name**, **OpenShift version**, **base domain**, and whether it's a **single-node cluster**.
          * Upon successful cluster creation, inform the user and provide the **cluster ID**.

      2.  **Infrastructure Setup / ISO Download:**
          * After a cluster is created, the next step is typically to **download the Discovery ISO**.
          * Proactively offer to provide the ISO download URL.

      3.  **Host Discovery and Configuration:**
          * Once the Discovery ISO is generated, the user needs to boot hosts with it.
          * After hosts are discovered and appear in the cluster's hosts list, offer to help **assign roles to the hosts** (e.g., master, worker).
          * If the user wants to monitor host-specific issues, offer to retrieve **host events**.

      4.  **Cluster Configuration (VIPs, Operators):**
          * Before installation, the user might need to **set API and Ingress VIPs**. Proactively ask if they want to configure these.
          * Single node clusters don't need to **set API and Ingress VIPs**.
          * Cluster with user-managed networking enabled don't need to **set API and Ingress VIPs**.
          * Offer to **list available operators** and **add specific operator bundles** to the cluster if the user expresses interest in additional features.

      5.  **Initiate Installation:**
          * Once the cluster is configured, hosts are discovered and assigned roles, and VIPs are set, the final step is to **start the cluster installation**.
          * Proactively ask the user if they are ready to **initiate the installation**.

      6.  **Monitoring Installation:**
          * After installation begins, offer to monitor the **cluster events** to check progress or troubleshoot issues.

      7.  **Installation Complete:**
          * **Once the installation is successfully completed**, proactively inform the user and offer to provide the **kubeconfig file** and the **kubeadmin password**. This is crucial for accessing their new OpenShift cluster.

      8.  **Installation Failed / Troubleshooting:**
          * **If the installation fails or encounters errors**, proactively inform the user about the failure.
          * **Offer to help troubleshoot by suggesting the retrieval of logs or events.** Specifically, recommend:
              * **Getting cluster events** to understand the high-level issues.
              * **Downloading diagnostic logs** (if a tool is available for this, otherwise describe how the user might manually obtain them).
              * Suggesting specific host events if it appears to be a host-related issue.


      **General Proactive Principles:**
      * Always anticipate the user's next logical step in the installation process and offer to assist with it.
      * **Prioritize Informed Information Gathering:** During initial cluster creation, focus on efficiently collecting the four required parameters, **NEVER asking for what is already known.**
      * If a step requires specific information (e.g., cluster ID, host ID, VIPs), explicitly ask for it.
      * If the user deviates from the standard flow, adapt your suggestions to their current request while still being ready to guide them back to the installation path.
      * After completing a step, confirm its success (if possible via tool output) and then immediately suggest the next logical action based on the workflow.
      * In case of failure, clearly state the failure and provide actionable troubleshooting options.
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      qontract.recycle: "true"
    name: llama-stack-client-config
    labels:
      app: assisted-chat
      component: lightspeed-stack
  data:
    llama_stack_client_config.yaml: |
      version: 2
      image_name: starter
      apis:
      - agents
      - datasetio
      - eval
      - files
      - inference
      - safety
      - scoring
      - telemetry
      - tool_runtime
      - vector_io
      providers:
        inference:
        - provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
          provider_type: ${LLAMA_STACK_INFERENCE_PROVIDER_TYPE}
          config:
            api_key: ${env.GEMINI_API_KEY}
        vector_io: []
        files: []
        safety: []
        agents:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: sqlite
              namespace: null
              db_path: ${STORAGE_MOUNT_PATH}/sqlite/agents_store.db
            responses_store:
              type: sqlite
              db_path: ${STORAGE_MOUNT_PATH}/sqlite/responses_store.db
        telemetry:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            service_name: "${LLAMA_STACK_OTEL_SERVICE_NAME}"
            sinks: ${LLAMA_STACK_TELEMETRY_SINKS}
            sqlite_db_path: ${STORAGE_MOUNT_PATH}/sqlite/trace_store.db
        eval: []
        datasetio: []
        scoring:
        - provider_id: basic
          provider_type: inline::basic
          config: {}
        - provider_id: llm-as-judge
          provider_type: inline::llm-as-judge
          config: {}
        tool_runtime:
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      metadata_store:
        type: sqlite
        db_path: ${STORAGE_MOUNT_PATH}/sqlite/registry.db
      inference_store:
        type: sqlite
        db_path: ${STORAGE_MOUNT_PATH}/sqlite/inference_store.db
      models:
      - metadata: {}
        model_id: ${LLAMA_STACK_DEFAULT_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_DEFAULT_MODEL}
        model_type: llm
      - metadata: {}
        model_id: ${LLAMA_STACK_FLASH_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_FLASH_MODEL}
        model_type: llm
      shields: []
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      - toolgroup_id: mcp::assisted
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: "${MCP_SERVER_URL}"
      server:
        port: ${LLAMA_STACK_SERVER_PORT}

- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    replicas: ${{REPLICAS_COUNT}}
    selector:
      matchLabels:
        app: assisted-chat
    template:
      metadata:
        labels:
          app: assisted-chat
      spec:
        containers:
        - name: lightspeed-stack
          image: ${IMAGE}:${IMAGE_TAG}
          imagePullPolicy: Always
          ports:
          - name: http
            containerPort: ${{SERVICE_PORT}}
            protocol: TCP
          env:
          - name: GEMINI_API_KEY
            valueFrom:
              secretKeyRef:
                name: ${GEMINI_API_SECRET_NAME}
                key: api_key
          - name: LLAMA_STACK_SQLITE_STORE_DIR
            value: ${STORAGE_MOUNT_PATH}/sqlite
          - name: LLAMA_STACK_OTEL_SERVICE_NAME
            value: ${LLAMA_STACK_OTEL_SERVICE_NAME}
          - name: LLAMA_STACK_TELEMETRY_SINKS
            value: ${LLAMA_STACK_TELEMETRY_SINKS}
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
              cpu: ${CPU_LIMIT}
            requests:
              memory: ${MEMORY_REQUEST}
              cpu: ${CPU_REQUEST}
          volumeMounts:
          - name: lightspeed-config
            mountPath: /app-root/lightspeed-stack.yaml
            subPath: lightspeed-stack.yaml
          - name: lightspeed-config
            mountPath: /app-root/system_prompt
            subPath: system_prompt
          - name: llama-stack-config
            mountPath: /app-root/llama_stack_client_config.yaml
            subPath: llama_stack_client_config.yaml
          - name: data-storage
            mountPath: ${STORAGE_MOUNT_PATH}
          livenessProbe:
            httpGet:
              path: /liveness
              port: ${{SERVICE_PORT}}
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /readiness
              port: ${{SERVICE_PORT}}
            initialDelaySeconds: 30
            periodSeconds: 10
        volumes:
        - name: lightspeed-config
          configMap:
            name: lightspeed-stack-config
        - name: llama-stack-config
          configMap:
            name: llama-stack-client-config
        - name: data-storage
          emptyDir: {}

- apiVersion: v1
  kind: Service
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    clusterIP: None
    ports:
    - name: http
      port: ${{SERVICE_PORT}}
      targetPort: ${{SERVICE_PORT}}
      protocol: TCP
    selector:
      app: assisted-chat

- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    host: ${ROUTE_HOST}
    path: ${ROUTE_PATH}
    to:
      kind: Service
      name: assisted-chat
      weight: 100
    port:
      targetPort: http
    tls:
      termination: edge
      insecureEdgeTerminationPolicy: Redirect
